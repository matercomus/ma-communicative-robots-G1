{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/matt/Dev/ma-communicative-robots-G1/combots-venv-new/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DslEWtJc7P9C"
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ca_8Nxy6MFbt",
    "outputId": "e2a505de-5805-4e88-e8f1-68c3413aba80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cltl.combot in /home/matt/Dev/ma-communicative-robots-G1/combots-venv-new/lib/python3.12/site-packages (1.1.0)\n",
      "Requirement already satisfied: emissor in /home/matt/Dev/ma-communicative-robots-G1/combots-venv-new/lib/python3.12/site-packages (from cltl.combot) (0.0.dev6)\n",
      "Requirement already satisfied: numpy~=1.20 in /home/matt/Dev/ma-communicative-robots-G1/combots-venv-new/lib/python3.12/site-packages (from emissor->cltl.combot) (1.26.4)\n",
      "Requirement already satisfied: marshmallow~=3.11 in /home/matt/Dev/ma-communicative-robots-G1/combots-venv-new/lib/python3.12/site-packages (from emissor->cltl.combot) (3.23.1)\n",
      "Requirement already satisfied: marshmallow-dataclass~=8.4 in /home/matt/Dev/ma-communicative-robots-G1/combots-venv-new/lib/python3.12/site-packages (from emissor->cltl.combot) (8.6.1)\n",
      "Requirement already satisfied: marshmallow-enum~=1.5 in /home/matt/Dev/ma-communicative-robots-G1/combots-venv-new/lib/python3.12/site-packages (from emissor->cltl.combot) (1.5.1)\n",
      "Requirement already satisfied: marshmallow-union~=0.1 in /home/matt/Dev/ma-communicative-robots-G1/combots-venv-new/lib/python3.12/site-packages (from emissor->cltl.combot) (0.1.15.post1)\n",
      "Requirement already satisfied: rdflib~=6.0 in /home/matt/Dev/ma-communicative-robots-G1/combots-venv-new/lib/python3.12/site-packages (from emissor->cltl.combot) (6.3.2)\n",
      "Requirement already satisfied: simplejson~=3.17 in /home/matt/Dev/ma-communicative-robots-G1/combots-venv-new/lib/python3.12/site-packages (from emissor->cltl.combot) (3.19.3)\n",
      "Requirement already satisfied: typeguard~=2.13 in /home/matt/Dev/ma-communicative-robots-G1/combots-venv-new/lib/python3.12/site-packages (from emissor->cltl.combot) (2.13.3)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/matt/Dev/ma-communicative-robots-G1/combots-venv-new/lib/python3.12/site-packages (from marshmallow~=3.11->emissor->cltl.combot) (24.2)\n",
      "Requirement already satisfied: typing-inspect<1.0,>=0.8.0 in /home/matt/Dev/ma-communicative-robots-G1/combots-venv-new/lib/python3.12/site-packages (from marshmallow-dataclass~=8.4->emissor->cltl.combot) (0.9.0)\n",
      "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /home/matt/Dev/ma-communicative-robots-G1/combots-venv-new/lib/python3.12/site-packages (from rdflib~=6.0->emissor->cltl.combot) (0.6.1)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /home/matt/Dev/ma-communicative-robots-G1/combots-venv-new/lib/python3.12/site-packages (from rdflib~=6.0->emissor->cltl.combot) (3.2.0)\n",
      "Requirement already satisfied: six in /home/matt/Dev/ma-communicative-robots-G1/combots-venv-new/lib/python3.12/site-packages (from isodate<0.7.0,>=0.6.0->rdflib~=6.0->emissor->cltl.combot) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/matt/Dev/ma-communicative-robots-G1/combots-venv-new/lib/python3.12/site-packages (from typing-inspect<1.0,>=0.8.0->marshmallow-dataclass~=8.4->emissor->cltl.combot) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/matt/Dev/ma-communicative-robots-G1/combots-venv-new/lib/python3.12/site-packages (from typing-inspect<1.0,>=0.8.0->marshmallow-dataclass~=8.4->emissor->cltl.combot) (4.12.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in /home/matt/Dev/ma-communicative-robots-G1/combots-venv-new/lib/python3.12/site-packages (1.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Matt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# sys.path.append(os.path.abspath('../combots-venv-new/lib/python3.12/site-packages'))\n",
    "%pip install cltl.combot --break-system-packages\n",
    "%pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Construct the relative path to the .env file\n",
    "env_path = os.path.join(current_dir, \"../.env\")\n",
    "# Load the .env file\n",
    "load_dotenv(env_path)\n",
    "\n",
    "\n",
    "# OpenAI API Key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gfSVLt26MFbt",
    "outputId": "b58093dc-947d-44cf-d83b-e39180940fa5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try 'pacman -S\n",
      "\u001b[31m   \u001b[0m python-xyz', where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Arch-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using 'python -m venv path/to/venv'.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Arch packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use 'pipx install xyz', which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have python-pipx\n",
      "\u001b[31m   \u001b[0m installed via pacman.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try 'pacman -S\n",
      "\u001b[31m   \u001b[0m python-xyz', where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Arch-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using 'python -m venv path/to/venv'.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Arch packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use 'pipx install xyz', which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have python-pipx\n",
      "\u001b[31m   \u001b[0m installed via pacman.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: cltl.combot in /home/matt/.local/lib/python3.12/site-packages (1.1.0)\n",
      "Requirement already satisfied: emissor in /home/matt/.local/lib/python3.12/site-packages (from cltl.combot) (0.0.dev6)\n",
      "Requirement already satisfied: numpy~=1.20 in /home/matt/.local/lib/python3.12/site-packages (from emissor->cltl.combot) (1.26.4)\n",
      "Requirement already satisfied: marshmallow~=3.11 in /home/matt/.local/lib/python3.12/site-packages (from emissor->cltl.combot) (3.23.1)\n",
      "Requirement already satisfied: marshmallow-dataclass~=8.4 in /home/matt/.local/lib/python3.12/site-packages (from emissor->cltl.combot) (8.6.1)\n",
      "Requirement already satisfied: marshmallow-enum~=1.5 in /home/matt/.local/lib/python3.12/site-packages (from emissor->cltl.combot) (1.5.1)\n",
      "Requirement already satisfied: marshmallow-union~=0.1 in /home/matt/.local/lib/python3.12/site-packages (from emissor->cltl.combot) (0.1.15.post1)\n",
      "Requirement already satisfied: rdflib~=6.0 in /home/matt/.local/lib/python3.12/site-packages (from emissor->cltl.combot) (6.3.2)\n",
      "Requirement already satisfied: simplejson~=3.17 in /home/matt/.local/lib/python3.12/site-packages (from emissor->cltl.combot) (3.19.2)\n",
      "Requirement already satisfied: typeguard~=2.13 in /home/matt/.local/lib/python3.12/site-packages (from emissor->cltl.combot) (2.13.3)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/lib/python3.12/site-packages (from marshmallow~=3.11->emissor->cltl.combot) (24.1)\n",
      "Requirement already satisfied: typing-inspect<1.0,>=0.8.0 in /home/matt/.local/lib/python3.12/site-packages (from marshmallow-dataclass~=8.4->emissor->cltl.combot) (0.9.0)\n",
      "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /home/matt/.local/lib/python3.12/site-packages (from rdflib~=6.0->emissor->cltl.combot) (0.6.1)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /home/matt/.local/lib/python3.12/site-packages (from rdflib~=6.0->emissor->cltl.combot) (3.1.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3.12/site-packages (from isodate<0.7.0,>=0.6.0->rdflib~=6.0->emissor->cltl.combot) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/matt/.local/lib/python3.12/site-packages (from typing-inspect<1.0,>=0.8.0->marshmallow-dataclass~=8.4->emissor->cltl.combot) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/lib/python3.12/site-packages (from typing-inspect<1.0,>=0.8.0->marshmallow-dataclass~=8.4->emissor->cltl.combot) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: line 1: apt-get: command not found\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ai2thor_colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      7\u001b[0m os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapt-get install xvfb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mai2thor_colab\u001b[39;00m\n\u001b[1;32m      9\u001b[0m ai2thor_colab\u001b[38;5;241m.\u001b[39mstart_xserver()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ai2thor_colab'"
     ]
    }
   ],
   "source": [
    "# Colab\n",
    "%pip install --upgrade ai2thor --quiet\n",
    "%pip install ai2thor-colab prior --upgrade &> /dev/null\n",
    "%pip install python-dotenv\n",
    "%pip install cltl.combot --break-system-packages\n",
    "import os\n",
    "\n",
    "os.system(\"apt-get install xvfb\")\n",
    "import ai2thor_colab\n",
    "\n",
    "ai2thor_colab.start_xserver()\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API Key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CIeQE8989ndG",
    "outputId": "7edb141a-db5d-4399-e075-5716b3d647a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AI2-THOR WARNING] There has been an update to ProcTHOR-10K that must be used with AI2-THOR version 5.0+. To use the new version of ProcTHOR-10K, please update AI2-THOR to version 5.0+ by running:\n",
      "    pip install --upgrade ai2thor\n",
      "Alternatively, to downgrade to the old version of ProcTHOR-10K, run:\n",
      "   prior.load_dataset(\"procthor-10k\", revision=\"ab3cacd0fc17754d4c080a3fd50b18395fae8647\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:02<00:00, 4440.68it/s]\n",
      "Loading val: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 5049.95it/s]\n",
      "Loading test: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 5238.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict(\n",
       "    train=Dataset(\n",
       "    dataset=procthor-dataset,\n",
       "    size=10000,\n",
       "    split=train\n",
       "),\n",
       "    val=Dataset(\n",
       "    dataset=procthor-dataset,\n",
       "    size=1000,\n",
       "    split=val\n",
       "),\n",
       "    test=Dataset(\n",
       "    dataset=procthor-dataset,\n",
       "    size=1000,\n",
       "    split=test\n",
       ")\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import prior\n",
    "\n",
    "dataset = prior.load_dataset(\"procthor-10k\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "cPurgiJ4U-C5"
   },
   "outputs": [],
   "source": [
    "house = dataset[\"train\"][11]  # CHOOSE HOUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "1Oa8abqULvzA"
   },
   "outputs": [],
   "source": [
    "from ai2thor.controller import Controller\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xv7Vm8BzR9dU",
    "outputId": "46e25a32-c7a3-48ce-8239-5bd836cab193"
   },
   "outputs": [],
   "source": [
    "controller = Controller(scene=house, visibilityDistance=10, width=750, height=750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "w51BWkAXBrqS"
   },
   "outputs": [],
   "source": [
    "event = controller.step(action=\"GetReachablePositions\")\n",
    "reachable_positions = event.metadata[\"actionReturn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "935AYZpDCaLq"
   },
   "outputs": [],
   "source": [
    "from openaiapi import analyze_image, analyze_prompt\n",
    "from utils import numpy_to_base64\n",
    "\n",
    "frame = controller.last_event.frame\n",
    "base64_string = numpy_to_base64(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Xr-ZujnCb_R"
   },
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "UvOXK5C2-N5v"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def teleport_in_front_of_object(\n",
    "    controller, object_position, reachable_positions, distance=1.0\n",
    "):\n",
    "    \"\"\"Teleports the agent to the closest reachable position in front of an object.\n",
    "\n",
    "    Args:\n",
    "      controller: The AI2Thor controller.\n",
    "      object_position: The position of the target object.\n",
    "      reachable_positions: A list of reachable positions in the scene.\n",
    "      distance: The desired distance in front of the object.\n",
    "\n",
    "    Returns:\n",
    "      The event after teleporting.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the target position in front of the object\n",
    "    target_position = {\n",
    "        \"x\": object_position[\"x\"] - distance,\n",
    "        \"y\": object_position[\"y\"],\n",
    "        \"z\": object_position[\"z\"],\n",
    "    }\n",
    "\n",
    "    # Find the closest reachable position\n",
    "    closest_position = None\n",
    "    min_distance = float(\"inf\")\n",
    "\n",
    "    for position in reachable_positions:\n",
    "        dist = math.sqrt(\n",
    "            (position[\"x\"] - target_position[\"x\"]) ** 2\n",
    "            + (position[\"z\"] - target_position[\"z\"]) ** 2\n",
    "        )\n",
    "        if dist < min_distance:\n",
    "            min_distance = dist\n",
    "            closest_position = position\n",
    "\n",
    "    # Calculate rotation towards the object\n",
    "    dx = object_position[\"x\"] - closest_position[\"x\"]\n",
    "    dz = object_position[\"z\"] - closest_position[\"z\"]\n",
    "    rotation = math.degrees(math.atan2(dx, dz))\n",
    "\n",
    "    # Teleport and rotate\n",
    "    event = controller.step(\n",
    "        action=\"Teleport\", position=closest_position, rotation=rotation\n",
    "    )\n",
    "\n",
    "    return event  # Return the event after adjusting view angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "nKgXHQjzOwmK"
   },
   "outputs": [],
   "source": [
    "def get_object_positions(controller, matched_object):\n",
    "    \"\"\"\n",
    "    Finds the positions of all visible objects of a specific type.\n",
    "\n",
    "    Args:\n",
    "      controller: The AI2Thor controller.\n",
    "      matched_object: The type of object to find (e.g., \"Painting\", \"Chair\", \"Table\").\n",
    "\n",
    "    Returns:\n",
    "      A list of positions for the specified object type.\n",
    "    \"\"\"\n",
    "    visible_objects = [\n",
    "        obj for obj in controller.last_event.metadata[\"objects\"] if obj[\"visible\"]\n",
    "    ]\n",
    "    objects_of_interest = [\n",
    "        obj for obj in visible_objects if obj[\"objectType\"] == matched_object\n",
    "    ]\n",
    "    object_positions = []\n",
    "    for obj in objects_of_interest:\n",
    "        # print(obj[\"name\"], obj[\"position\"])\n",
    "        object_positions.append(obj[\"position\"])\n",
    "    return object_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "pbYQyvUq6eCo"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def interactive_object_match(\n",
    "    api_key: str,\n",
    "    human_object_description: str,\n",
    "    unique_object_list: list,\n",
    "    HUMAN: str,\n",
    "    AGENT: str,\n",
    "    leolaniClient,\n",
    "):\n",
    "    \"\"\"\n",
    "    Interactively matches a human description of an object to one from a given list using an LLM.\n",
    "    The function continues to refine guesses based on user confirmation or denial.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): The API key for accessing the LLM.\n",
    "        human_object_description (str): A description of the object to match.\n",
    "        unique_object_list (list): The list of unique objects to match against.\n",
    "\n",
    "    Returns:\n",
    "        str: The confirmed object from the user.\n",
    "        list: The matched object(s) from the list based on the LLM's response.\n",
    "    \"\"\"\n",
    "\n",
    "    def ask_llm(description: str, objects: list) -> str:\n",
    "        \"\"\"Helper function to query the LLM for matching the description.\"\"\"\n",
    "        object_list_str = \", \".join(objects)\n",
    "        prompt = (\n",
    "            f\"Imagine you are tasked with identifying an object from a given list based on its description. \"\n",
    "            f\"The list of objects is: {object_list_str}. \"\n",
    "            f\"Your task is to match the following description to one or more objects from the list: \\n\"\n",
    "            f\"'{description}'\\n\\n\"\n",
    "            \"If you have a single best guess, respond with: 'To be sure, would you describe your object as {object}? '\\n\"\n",
    "            \"If you are unsure and need clarification between a few options, respond with: \"\n",
    "            \"'To be sure, would you describe your object as {object1} or {object2}? '\"\n",
    "            \"Only use objects from the list.\"\n",
    "        )\n",
    "        # Make sure the response is extracted correctly from the LLM\n",
    "        llm_response = analyze_prompt(api_key=api_key, prompt=prompt)\n",
    "\n",
    "        if isinstance(llm_response, tuple):\n",
    "            llm_response = llm_response[0]\n",
    "\n",
    "        if (\n",
    "            isinstance(llm_response, list) and llm_response\n",
    "        ):  # Check if it's a non-empty list\n",
    "            llm_response = llm_response[0]  # Access the first element if it's a list\n",
    "\n",
    "        if (\n",
    "            isinstance(llm_response, dict)\n",
    "            and \"choices\" in llm_response\n",
    "            and llm_response[\"choices\"]\n",
    "        ):\n",
    "            return llm_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        return llm_response  # Return raw response if format is unexpected\n",
    "\n",
    "    current_description = human_object_description\n",
    "\n",
    "    while True:\n",
    "        # Query the LLM for a guess\n",
    "        response = ask_llm(current_description, unique_object_list)\n",
    "        leolaniClient._add_utterance(AGENT, response)\n",
    "        print(f\"{AGENT}>{response}\")\n",
    "\n",
    "        # Extract the matched object(s) from the response\n",
    "        matched_objects = re.findall(\n",
    "            r\"\\b(\" + \"|\".join(map(re.escape, unique_object_list)) + r\")\\b\", response\n",
    "        )\n",
    "\n",
    "        if matched_objects:\n",
    "            # Ask the user for confirmation or denial\n",
    "            confirmation_prompt = f\"Is this correct? (yes/no): \"\n",
    "            leolaniClient._add_utterance(AGENT, confirmation_prompt)\n",
    "            print(f\"{AGENT}>{confirmation_prompt}\")\n",
    "            user_input = input().strip().lower()\n",
    "            leolaniClient._add_utterance(HUMAN, user_input)\n",
    "            print(f\"{HUMAN}>{user_input}\")\n",
    "\n",
    "            if user_input == \"yes\":\n",
    "                success_message = \"Great! Object successfully matched.\"\n",
    "                leolaniClient._add_utterance(AGENT, success_message)\n",
    "                print(f\"{AGENT}>{success_message}\")\n",
    "                return (\n",
    "                    matched_objects[0] if len(matched_objects) == 1 else matched_objects\n",
    "                )\n",
    "            elif user_input == \"no\":\n",
    "                refine_message = \"Let's refine the search. Can you provide more details or clarify the description?\"\n",
    "                leolaniClient._add_utterance(AGENT, refine_message)\n",
    "                print(f\"{AGENT}>{refine_message}\")\n",
    "                clarifying_question = input().strip()\n",
    "                leolaniClient._add_utterance(HUMAN, clarifying_question)\n",
    "                print(f\"{HUMAN}>{clarifying_question}\")\n",
    "                current_description = (\n",
    "                    clarifying_question  # Update the description with the new input\n",
    "                )\n",
    "            else:\n",
    "                error_message = \"Please respond with 'yes' or 'no'.\"\n",
    "                leolaniClient._add_utterance(AGENT, error_message)\n",
    "                print(f\"{AGENT}>{error_message}\")\n",
    "        else:\n",
    "            error_message = (\n",
    "                \"I couldn't find a matching object. Can you provide more details?\"\n",
    "            )\n",
    "            leolaniClient._add_utterance(AGENT, error_message)\n",
    "            print(f\"{AGENT}>{error_message}\")\n",
    "            clarifying_question = input().strip()\n",
    "            leolaniClient._add_utterance(HUMAN, clarifying_question)\n",
    "            print(f\"{HUMAN}>{clarifying_question}\")\n",
    "            current_description += \" \" + clarifying_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "Oi4GNic-9wge"
   },
   "outputs": [],
   "source": [
    "unique_object_list = [\n",
    "    \"AlarmClock\",\n",
    "    \"AluminumFoil\",\n",
    "    \"Apple\",\n",
    "    \"ArmChair\",\n",
    "    \"BaseballBat\",\n",
    "    \"BasketBall\",\n",
    "    \"Bathtub\",\n",
    "    \"BathtubBasin\",\n",
    "    \"Bed\",\n",
    "    \"Blinds\",\n",
    "    \"Book\",\n",
    "    \"Boots\",\n",
    "    \"Bottle\",\n",
    "    \"Bowl\",\n",
    "    \"Box\",\n",
    "    \"Bread\",\n",
    "    \"ButterKnife\",\n",
    "    \"Cabinet\",\n",
    "    \"Candle\",\n",
    "    \"CD\",\n",
    "    \"CellPhone\",\n",
    "    \"Chair\",\n",
    "    \"Cloth\",\n",
    "    \"CoffeeMachine\",\n",
    "    \"CoffeeTable\",\n",
    "    \"CounterTop\",\n",
    "    \"CreditCard\",\n",
    "    \"Cup\",\n",
    "    \"Curtains\",\n",
    "    \"Desk\",\n",
    "    \"DeskLamp\",\n",
    "    \"Desktop\",\n",
    "    \"DiningTable\",\n",
    "    \"DishSponge\",\n",
    "    \"DogBed\",\n",
    "    \"Drawer\",\n",
    "    \"Dresser\",\n",
    "    \"Dumbbell\",\n",
    "    \"Egg\",\n",
    "    \"Faucet\",\n",
    "    \"Floor\",\n",
    "    \"FloorLamp\",\n",
    "    \"Footstool\",\n",
    "    \"Fork\",\n",
    "    \"Fridge\",\n",
    "    \"GarbageBag\",\n",
    "    \"GarbageCan\",\n",
    "    \"HandTowel\",\n",
    "    \"HandTowelHolder\",\n",
    "    \"HousePlant\",\n",
    "    \"Kettle\",\n",
    "    \"KeyChain\",\n",
    "    \"Knife\",\n",
    "    \"Ladle\",\n",
    "    \"Laptop\",\n",
    "    \"LaundryHamper\",\n",
    "    \"Lettuce\",\n",
    "    \"LightSwitch\",\n",
    "    \"Microwave\",\n",
    "    \"Mirror\",\n",
    "    \"Mug\",\n",
    "    \"Newspaper\",\n",
    "    \"Ottoman\",\n",
    "    \"Painting\",\n",
    "    \"Pan\",\n",
    "    \"PaperTowelRoll\",\n",
    "    \"Pen\",\n",
    "    \"Pencil\",\n",
    "    \"PepperShaker\",\n",
    "    \"Pillow\",\n",
    "    \"Plate\",\n",
    "    \"Plunger\",\n",
    "    \"Poster\",\n",
    "    \"Pot\",\n",
    "    \"Potato\",\n",
    "    \"RemoteControl\",\n",
    "    \"RoomDecor\",\n",
    "    \"Safe\",\n",
    "    \"SaltShaker\",\n",
    "    \"ScrubBrush\",\n",
    "    \"Shelf\",\n",
    "    \"ShelvingUnit\",\n",
    "    \"ShowerCurtain\",\n",
    "    \"ShowerDoor\",\n",
    "    \"ShowerGlass\",\n",
    "    \"ShowerHead\",\n",
    "    \"SideTable\",\n",
    "    \"Sink\",\n",
    "    \"SinkBasin\",\n",
    "    \"SoapBar\",\n",
    "    \"SoapBottle\",\n",
    "    \"Sofa\",\n",
    "    \"Spatula\",\n",
    "    \"Spoon\",\n",
    "    \"SprayBottle\",\n",
    "    \"Statue\",\n",
    "    \"Stool\",\n",
    "    \"StoveBurner\",\n",
    "    \"StoveKnob\",\n",
    "    \"TableTopDecor\",\n",
    "    \"TargetCircle\",\n",
    "    \"TeddyBear\",\n",
    "    \"Television\",\n",
    "    \"TennisRacket\",\n",
    "    \"TissueBox\",\n",
    "    \"Toaster\",\n",
    "    \"Toilet\",\n",
    "    \"ToiletPaper\",\n",
    "    \"ToiletPaperHanger\",\n",
    "    \"Tomato\",\n",
    "    \"Towel\",\n",
    "    \"TowelHolder\",\n",
    "    \"TVStand\",\n",
    "    \"VacuumCleaner\",\n",
    "    \"Vase\",\n",
    "    \"Watch\",\n",
    "    \"WateringCan\",\n",
    "    \"Window\",\n",
    "    \"WineBottle\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DNPvIi-9ndS"
   },
   "source": [
    "# Test Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "jsOTU7pCVdcA"
   },
   "outputs": [],
   "source": [
    "# adding to the system path\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(\"../emissor_chat\"))\n",
    "\n",
    "from leolani_client import Action, LeolaniChatClient\n",
    "\n",
    "emissor_path = \"./emissor\"\n",
    "AGENT = \"Ai2Thor\"\n",
    "HUMAN = \"Human\"\n",
    "leolaniClient = LeolaniChatClient(emissor_path=emissor_path, agent=AGENT, human=HUMAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ezATi5dbMFb3",
    "outputId": "478c4ea8-8b8a-4037-db26-959a16db050a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ai2Thor>Hi Human. What do you see in the room?\n"
     ]
    }
   ],
   "source": [
    "utterance = f\"Hi {HUMAN}. What do you see in the room?\"\n",
    "print(AGENT + \">\" + utterance)\n",
    "leolaniClient._add_utterance(AGENT, utterance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "V6jOykszMFb3"
   },
   "outputs": [],
   "source": [
    "# grab img of an item to look for\n",
    "\n",
    "# example: look for a tv in a room\n",
    "# event = controller.step(action=\"Teleport\", position={'x': 2.75, 'y': 0.9009997844696045, 'z': 1.0}  with rotation 209)\n",
    "# Image.fromarray(event.frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "up-KwAtd9ndS"
   },
   "outputs": [],
   "source": [
    "human_room_description = (\n",
    "    \"there is a table. 5 chairs. wthere is a window. its probably a living room.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YyrWK2lpMFb3",
    "outputId": "be21c31e-51b4-4430-bc31-e28b876da4c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human>there is a table. 5 chairs. wthere is a window. its probably a living room.\n"
     ]
    }
   ],
   "source": [
    "print(HUMAN + \">\" + human_room_description)\n",
    "leolaniClient._add_utterance(HUMAN, human_room_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "J0CAYrjg9ndT"
   },
   "outputs": [],
   "source": [
    "# claryfying questions\n",
    "claryfying_questions_response = analyze_prompt(\n",
    "    api_key=api_key,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    prompt=f\"Imagine you are a robot who needs to be on a exact location as the point of view that the human has. After a while, the human can no longer see this image. The human will most likely describe a room from memory. The human will most likely describe a few objects and maybe some other attributes, like colours of objects. Your task is to ask claryfing questions about the room and objects so that you (the robot) has the highest chance of finding where the human was standing. Remember, ask the questions as if you were directly talking to the human. Try not to ask for too much details and dont ask for too much; remember, the human has to describe the image from memory, so only ask what you deem most important. \\n Human description: {human_room_description}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'id': 'chatcmpl-Aa1b9FAFXdVPdlj1z7c4ebcwAwanH',\n",
       "  'object': 'chat.completion',\n",
       "  'created': 1733149315,\n",
       "  'model': 'gpt-4o-mini-2024-07-18',\n",
       "  'choices': [{'index': 0,\n",
       "    'message': {'role': 'assistant',\n",
       "     'content': 'Thank you for your description! To help me find the exact location, could you please tell me more about the table? What shape is it, and what color or material is it made of? Also, do you remember anything about the window, like its size or any curtains?',\n",
       "     'refusal': None},\n",
       "    'logprobs': None,\n",
       "    'finish_reason': 'stop'}],\n",
       "  'usage': {'prompt_tokens': 184,\n",
       "   'completion_tokens': 56,\n",
       "   'total_tokens': 240,\n",
       "   'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
       "   'completion_tokens_details': {'reasoning_tokens': 0,\n",
       "    'audio_tokens': 0,\n",
       "    'accepted_prediction_tokens': 0,\n",
       "    'rejected_prediction_tokens': 0}},\n",
       "  'system_fingerprint': 'fp_0705bf87c0'},\n",
       " 1.1566174030303955)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claryfying_questions_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66IEJwOnMFb4",
    "outputId": "d1257130-5259-470e-b04e-fd3d889beb0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ai2Thor>Thank you for your description! To help me find the exact location, could you please tell me more about the table? What shape is it, and what color or material is it made of? Also, do you remember anything about the window, like its size or any curtains?\n"
     ]
    }
   ],
   "source": [
    "utterance = claryfying_questions_response[0][\"choices\"][0][\"message\"][\"content\"]\n",
    "print(AGENT + \">\" + utterance)\n",
    "leolaniClient._add_utterance(AGENT, utterance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2zJDwKz1MFb4",
    "outputId": "95237741-2b6e-4d94-a773-5b9500f8c456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human>The table is blue, chairs are all black. The window is on the left wall in the same corner as th balcony doors.\n"
     ]
    }
   ],
   "source": [
    "human_room_description_clarified = \"The table is blue, chairs are all black. The window is on the left wall in the same corner as th balcony doors.\"\n",
    "print(HUMAN + \">\" + human_room_description_clarified)\n",
    "leolaniClient._add_utterance(HUMAN, human_room_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8vn0lgHmMFb4",
    "outputId": "34d64d75-eea2-486a-f59b-6d30afaf7ff5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ai2Thor>Describe the object I should look for.\n"
     ]
    }
   ],
   "source": [
    "utterance = \"Describe the object I should look for.\"\n",
    "print(AGENT + \">\" + utterance)\n",
    "leolaniClient._add_utterance(AGENT, utterance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "TIhZRTY_LRUS"
   },
   "outputs": [],
   "source": [
    "human_obj_description = \"It's a dark painting with trees a moon. some clouds, a river.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zkA_CO4yMFb4",
    "outputId": "e77acc53-d733-49fd-afc6-9745ff797a7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human>It's a dark painting with trees a moon. some clouds, a river.\n"
     ]
    }
   ],
   "source": [
    "print(HUMAN + \">\" + human_obj_description)\n",
    "leolaniClient._add_utterance(HUMAN, human_room_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jg39BgRdPE0A",
    "outputId": "3f35014b-3ffa-4457-98d3-1f2581f4f171"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ai2Thor>To be sure, would you describe your object as Painting?\n",
      "Ai2Thor>Is this correct? (yes/no): \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " no\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human>no\n",
      "Ai2Thor>Let's refine the search. Can you provide more details or clarify the description?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " plant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human>plant\n",
      "Ai2Thor>To be sure, would you describe your object as HousePlant?\n",
      "Ai2Thor>Is this correct? (yes/no): \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human>yes\n",
      "Ai2Thor>Great! Object successfully matched.\n"
     ]
    }
   ],
   "source": [
    "# this is where the interactive object match comes :\n",
    "# based on \"dark painting with trees ...\", did you mean \"Painting\"?\n",
    "\n",
    "human_object_description = human_obj_description\n",
    "matched_object = interactive_object_match(\n",
    "    api_key=api_key,\n",
    "    human_object_description=human_object_description,\n",
    "    unique_object_list=unique_object_list,\n",
    "    HUMAN=HUMAN,\n",
    "    AGENT=AGENT,\n",
    "    leolaniClient=leolaniClient,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HousePlant'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Gz8CnenWUCw",
    "outputId": "572d1156-2bf6-4df9-f343-56d17f8f8622"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teleporting the agent to {'x': 2.75, 'y': 0.9009997844696045, 'z': 4.75}  with rotation 235\n"
     ]
    }
   ],
   "source": [
    "# Teleport somewhere random\n",
    "import random\n",
    "\n",
    "position = random.choice(reachable_positions)\n",
    "rotation = random.choice(range(360))\n",
    "print(\"Teleporting the agent to\", position, \" with rotation\", rotation)\n",
    "\n",
    "event = controller.step(action=\"Teleport\", position=position, rotation=rotation)\n",
    "\n",
    "# Image.fromarray(event.frame) # image for clearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rKq23C6qMFb4",
    "outputId": "221d8dbd-eadc-41db-95f9-496aa267c2f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ai2Thor>As a robot, I see a blue table with five black chairs. There's a window on the left wall next to the balcony doors. This appears to match the description of the room I'm looking for. I believe I'm in the correct room.\n"
     ]
    }
   ],
   "source": [
    "# location classificaiton:\n",
    "\n",
    "# chatgpt, do you think you are in the correct room based on human_room_description + human_room description_clarified? (maybe rotate 360 degrees, but also have to analyze 4 images then)\n",
    "\n",
    "room_classifcation = analyze_image(\n",
    "    base64_string,\n",
    "    api_key=api_key,\n",
    "    prompt=f\"Imagine you are a robot looking for a certain room. Describe the room shortly. Do you think you are in the correct room based on the following description? Description: {human_room_description}, even further description: {human_room_description_clarified}, speak to me as if you have the robots point of view. Be consise in your answer.\",\n",
    ")\n",
    "utterance = room_classifcation[0][\"choices\"][0][\"message\"][\"content\"]\n",
    "print(AGENT + \">\" + utterance)\n",
    "leolaniClient._add_utterance(AGENT, utterance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGqRzS5sgBpQ"
   },
   "outputs": [],
   "source": [
    "# TO DO: Decide what to do: teleport to another room, teleport to object instance (if there is one) or lookleft/lookright etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q6zxeCqCOvrv",
    "outputId": "28f009f4-a598-422e-e51e-d7a1c86c7707"
   },
   "outputs": [],
   "source": [
    "# Metadata\n",
    "\n",
    "# i see x instances of matched_object\n",
    "# teleport to matched_object[0]\n",
    "# use chatgpt to describe image\n",
    "# do so until image is found or no instances left\n",
    "\n",
    "# matched_object = \"Painting\" # placeholder\n",
    "\n",
    "object_positions = get_object_positions(controller, matched_object)\n",
    "\n",
    "for i in range(len(object_positions)):\n",
    "    position = object_positions[i]\n",
    "    event = teleport_in_front_of_object(controller, position, reachable_positions)\n",
    "\n",
    "    description = analyze_image(\n",
    "        base64_string,\n",
    "        api_key=api_key,\n",
    "        prompt=f\"describe {matched_object} in great detail\",\n",
    "    )\n",
    "    utterance = description[0][\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    print(AGENT + \">\" + utterance + \"Was this the item you were looking for?\")\n",
    "    leolaniClient._add_utterance(\n",
    "        AGENT, utterance + \"Was this the item you were looking for?\"\n",
    "    )\n",
    "    if input(\"Type yes if so...\") == \"yes\":\n",
    "        print(HUMAN + \">\" + \"yes\")\n",
    "        leolaniClient._add_utterance(HUMAN, \"yes\")\n",
    "        break\n",
    "    else:\n",
    "        print(HUMAN + \">\" + \"no\")\n",
    "        leolaniClient._add_utterance(HUMAN, \"no\")\n",
    "        continue\n",
    "\n",
    "print(\"Teleport to new room if item isn't found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "htVsg7BwQDVN"
   },
   "outputs": [],
   "source": [
    "# TO DO: if no more instances, teleport to new room\n",
    "\n",
    "# Look at the furthest away coordinate compared to the visited coordinates, just take the eucliadian distances of each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dVLLHlrYMFb5"
   },
   "outputs": [],
   "source": [
    "##### After completion, we save the scenario in the defined emissor folder.\n",
    "leolaniClient._save_scenario()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbLveC1jLKvS"
   },
   "source": [
    "# EXAMPLE IMAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFf51WX4DTwx"
   },
   "source": [
    "RANDOM TELEPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UthmJigQDLrk"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "position = random.choice(reachable_positions)\n",
    "rotation = random.choice(range(360))\n",
    "print(\"Teleporting the agent to\", position, \" with rotation\", rotation)\n",
    "\n",
    "event = controller.step(action=\"Teleport\", position=position, rotation=rotation)\n",
    "Image.fromarray(event.frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D40jijBJDYRQ"
   },
   "source": [
    "IMAGE WITH 2 PAINTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZsG_wobDZtJ"
   },
   "outputs": [],
   "source": [
    "event = controller.step(\n",
    "    action=\"Teleport\",\n",
    "    position={\"x\": 2.5, \"y\": 0.9009997844696045, \"z\": 4.5},\n",
    "    rotation=80,\n",
    ")\n",
    "Image.fromarray(event.frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgFX4IJ1DhhQ"
   },
   "source": [
    "#TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7d42tDqvDvjo"
   },
   "source": [
    "ROTATING AND REMEMBERING OBJECT LOCATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HztOs1ffDkK7"
   },
   "outputs": [],
   "source": [
    "visible_objects = [\n",
    "    obj for obj in controller.last_event.metadata[\"objects\"] if obj[\"visible\"]\n",
    "]\n",
    "paintings = [obj for obj in visible_objects if obj[\"objectType\"] == \"Painting\"]\n",
    "all_painting_positions = []  # To store all painting positions\n",
    "\n",
    "for _ in range(3):  # Rotate three times\n",
    "    # Get visible paintings and their positions\n",
    "    current_painting_positions = []\n",
    "    for painting in paintings:\n",
    "        print(painting[\"name\"], painting[\"position\"])\n",
    "        current_painting_positions.append(painting[\"position\"])\n",
    "\n",
    "    # Add current painting positions to the overall list\n",
    "    all_painting_positions.extend(current_painting_positions)\n",
    "\n",
    "    # Rotate the agent\n",
    "    controller.step(\"RotateRight\")\n",
    "\n",
    "    # Update visible objects and paintings for the next iteration\n",
    "    visible_objects = [\n",
    "        obj for obj in controller.last_event.metadata[\"objects\"] if obj[\"visible\"]\n",
    "    ]\n",
    "    paintings = [obj for obj in visible_objects if obj[\"objectType\"] == \"Painting\"]\n",
    "\n",
    "print(\"All painting positions in the room:\", all_painting_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBWGmiSOD6-g"
   },
   "source": [
    "LOOK UP / DOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ebhg0NmD6Ao"
   },
   "outputs": [],
   "source": [
    "# controller.step(\"LookDown\")\n",
    "# Image.fromarray(controller.last_event.frame)\n",
    "\n",
    "# after we take look up or down we should return to the original state, THIS IS IMPORTANT OTHERWISE IT WILL MESS WITH TELEPORTING"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (combots-venv-new)",
   "language": "python",
   "name": "combots-venv-new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
